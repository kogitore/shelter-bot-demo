import asyncio
from datetime import datetime
import pandas as pd
import psycopg
import os
import requests
from math import ceil
import time
import re
from dotenv import load_dotenv

load_dotenv()
# -----------------------------------------------
# 1. å»ºç«‹è³‡æ–™åº«
# -----------------------------------------------

async def create_table():
    database_url = os.environ.get("DATABASE_URL")
    if not database_url:
        raise ValueError("DATABASE_URL ç’°å¢ƒè®Šæ•¸æœªè¨­å®š")

    create_table_query = """
    CREATE EXTENSION IF NOT EXISTS postgis;

    CREATE TABLE IF NOT EXISTS shelters (
    id SERIAL PRIMARY KEY,
    shelter_code VARCHAR(20) UNIQUE,
    name TEXT NOT NULL,
    city TEXT,
    district TEXT,
    address TEXT,
    capacity INT,
    area_m2 NUMERIC,
    disasters TEXT[],           
    relief_station BOOLEAN,
    barrier_free BOOLEAN,
    indoor BOOLEAN,
    outdoor BOOLEAN,
    service_area TEXT,
    memo TEXT,
    geom GEOGRAPHY(POINT, 4326),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
    );
    """

    async with await psycopg.AsyncConnection.connect(database_url) as conn:
        async with conn.cursor() as cur:
            await cur.execute(create_table_query)
            await conn.commit()
            print("shelters table created successfully")

# -----------------------------------------------
# 2. å°åŒ—å¸‚é¿é›£æ‰€ API æŠ“å–(å«å¿«å–)
#    GET https://data.taipei/api/v1/dataset/4c92dbd4-d259-495a-8390-52628119a4dd?scope=resourceAquire&limit=1000&offset=0
#    ç›®å‰åªæœ‰401ç­†è³‡æ–™
# -----------------------------------------------

DATA_PATH = os.path.join(os.path.dirname(__file__), "documents", "taipei_shelters.csv")
API_URL = "https://data.taipei/api/v1/dataset/4c92dbd4-d259-495a-8390-52628119a4dd"
API_PARAMS = {
    "scope": "resourceAquire",
    "resource_id": "4c92dbd4-d259-495a-8390-52628119a4dd",
    "limit": 1000,
    "offset": 0
}

def fetch_remote_data(limit=10):
    params = API_PARAMS.copy()
    params["limit"] = limit
    resp = requests.get(API_URL, params=params, timeout=15)
    data = resp.json()
    return data.get("result", {}).get("results", [])

def get_latest_import_date(records):
    dates = []
    for r in records:
        try:
            d = r.get("_importdate", {}).get("date")
            if d:
                dates.append(datetime.fromisoformat(d))
        except Exception:
            continue
    return max(dates) if dates else None

def check_update_needed():
    """æª¢æŸ¥è³‡æ–™æ˜¯å¦éœ€è¦æ›´æ–°"""
    if not os.path.exists(DATA_PATH):
        print("ğŸ“‚ æ‰¾ä¸åˆ°æœ¬åœ° CSVï¼Œé–‹å§‹ä¸‹è¼‰...")
        return True

    df_local = pd.read_csv(DATA_PATH)
    local_latest = pd.to_datetime(df_local["_importdate"]).max()

    online_records = fetch_remote_data(limit=10)
    remote_latest = get_latest_import_date(online_records)

    if remote_latest and remote_latest > local_latest:
        print(f"ç™¼ç¾æ–°ç‰ˆæœ¬ï¼šé ç«¯ {remote_latest} > æœ¬åœ° {local_latest}")
        return True
    else:
        print(f"è³‡æ–™å·²æ˜¯æœ€æ–° ({local_latest})ï¼Œä¸éœ€æ›´æ–°ã€‚")
        return False

def save_to_csv(records):
    """å„²å­˜è³‡æ–™åˆ° CSV"""
    df = pd.DataFrame(records)
    df["_importdate"] = df["_importdate"].apply(lambda x: x.get("date") if isinstance(x, dict) else x)
    os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)
    df.to_csv(DATA_PATH, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ å·²å„²å­˜ {len(df)} ç­†è³‡æ–™åˆ° {DATA_PATH}")

def load_cached_data():
    """è®€å–æœ¬åœ° CSV"""
    if not os.path.exists(DATA_PATH):
        return []
    df = pd.read_csv(DATA_PATH)
    return df.to_dict(orient="records")

def fetch_taipei_shelters():
    """ç²å–å°åŒ—å¸‚é¿é›£æ‰€è³‡æ–™ï¼ˆè‡ªå‹•å¿«å–æ›´æ–°ï¼‰"""
    if check_update_needed():
        print("ğŸ“¡ å¾ API ä¸‹è¼‰æœ€æ–°è³‡æ–™ä¸­...")
        records = fetch_remote_data(limit=1000)
        save_to_csv(records)
        return records
    else:
        print("ğŸ“„ ä½¿ç”¨æœ¬åœ°å¿«å–è³‡æ–™ã€‚")
        return load_cached_data()


# -----------------------------------------------
# 3. OpenStreetMap Geocoding (Nominatim)
# -----------------------------------------------

def geocode_address(row):
    """ä½¿ç”¨ OSM Nominatim å°‡å°ç£åœ°å€è½‰æ›æˆ (lat, lon)"""
    try:
        url = "https://nominatim.openstreetmap.org/search"
        city = row.get("ç¸£å¸‚") or ""
        district = row.get("é„‰é®") or ""
        address = row.get("é–€ç‰Œåœ°å€") or ""

        # ---- åœ°å€æ­£è¦åŒ–è™•ç† ----
        pattern = r"(.+?)([ï¼-ï¼™0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+è™Ÿ)$"
        match = re.search(pattern, address)
        if match:
            road = match.group(1)
            number = match.group(2)
            formatted_address = f"{number}, {road.strip()}, {district}, {city}, Taiwan"
        else:
            formatted_address = f"{address}, {district}, {city}, Taiwan"

        # ---- æŸ¥è©¢ OSM ----
        params = {
            "q": formatted_address,
            "format": "json",
            "limit": 1,
            "countrycodes": "tw"
        }
        headers = {"User-Agent": "TaipeiShelterBot/1.0"}
        resp = requests.get(url, params=params, headers=headers, timeout=10)
        data = resp.json()

        if data:
            lat = float(data[0]["lat"])
            lon = float(data[0]["lon"])
            print(f"âœ… {formatted_address} â†’ POINT({lon} {lat})")
            return lat, lon
        else:
            print(f"âš ï¸ Skip {formatted_address} â€” no coordinates found.")
            return None, None

    except Exception as e:
        formatted_address = row.get("é–€ç‰Œåœ°å€", "")
        print(f"âš ï¸ Geocode failed for {formatted_address}: {e}")
        return None, None

# ------------------------------------------------------
# 4. æ‰¹æ¬¡æŸ¥è©¢æ‰€æœ‰é¿é›£æ‰€åº§æ¨™ä¸¦ä¿å­˜åˆ° CSV
# ------------------------------------------------------

def geocode_and_save(records):
    """æ‰¹æ¬¡æŸ¥è©¢æ‰€æœ‰é¿é›£æ‰€åº§æ¨™ï¼Œé–“éš” 0.5 ç§’é¿å…è¢«å°é–ï¼Œä¸¦ä¿å­˜åˆ° CSV"""
    enriched_records = []
    total_geocoded = 0
    
    for i, row in enumerate(records, 1):
        # æª¢æŸ¥æ˜¯å¦å·²æœ‰åº§æ¨™è³‡æ–™
        if row.get("lat") and row.get("lon"):
            enriched_records.append(row)
            print(f"ğŸ“ {i}. {row.get('åç¨±')} â†’ ä½¿ç”¨å·²æœ‰åº§æ¨™ POINT({row.get('lon')} {row.get('lat')})")
        else:
            # é€²è¡Œåœ°ç†ç·¨ç¢¼
            lat, lon = geocode_address(row)
            if lat and lon:
                row["lat"] = lat
                row["lon"] = lon
                enriched_records.append(row)
                total_geocoded += 1
            time.sleep(0.5)  # åªæœ‰åœ¨å¯¦éš›æŸ¥è©¢æ™‚æ‰å»¶é²
    
    print(f"\nâœ… æˆåŠŸå–å¾— {len(enriched_records)} ç­†åº§æ¨™ï¼ˆå…± {len(records)} ç­†ï¼Œæ–°æŸ¥è©¢ {total_geocoded} ç­†ï¼‰")
    
    # å°‡åŒ…å«åº§æ¨™çš„è³‡æ–™ä¿å­˜å› CSV
    if enriched_records:
        save_to_csv(enriched_records)
        print(f"ğŸ’¾ å·²æ›´æ–° CSV æª”æ¡ˆï¼ŒåŒ…å«åº§æ¨™è³‡è¨Š")
    
    return enriched_records

def get_records_with_coordinates():
    """ç²å–åŒ…å«åº§æ¨™çš„é¿é›£æ‰€è³‡æ–™"""
    # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æŠ“å– API è³‡æ–™
    if check_update_needed():
        print("ğŸ“¡ å¾ API ä¸‹è¼‰æœ€æ–°è³‡æ–™ä¸­...")
        records = fetch_remote_data(limit=1000)
        # é€²è¡Œåœ°ç†ç·¨ç¢¼ä¸¦ä¿å­˜
        return geocode_and_save(records)
    else:
        print("ğŸ“„ ä½¿ç”¨æœ¬åœ°å¿«å–è³‡æ–™ã€‚")
        cached_records = load_cached_data()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™å®Œæ•´æ€§å•é¡Œ
        api_records = fetch_remote_data(limit=1000)
        api_count = len(api_records)
        cached_count = len(cached_records)
        
        if cached_count < api_count:
            print(f"âš ï¸ å¿«å–è³‡æ–™ä¸å®Œæ•´ï¼šæœ¬åœ° {cached_count} ç­† < é ç«¯ {api_count} ç­†")
            print("ğŸ“¡ é‡æ–°ä¸‹è¼‰ä¸¦è™•ç†å®Œæ•´è³‡æ–™...")
            return geocode_and_save(api_records)
        
        # æª¢æŸ¥å¿«å–è³‡æ–™ä¸­æ˜¯å¦æœ‰ç¼ºå°‘åº§æ¨™çš„è¨˜éŒ„
        missing_coords = [r for r in cached_records if not (r.get("lat") and r.get("lon"))]
        if missing_coords:
            print(f"ğŸ” ç™¼ç¾ {len(missing_coords)} ç­†è³‡æ–™ç¼ºå°‘åº§æ¨™ï¼Œé€²è¡Œåœ°ç†ç·¨ç¢¼...")
            return geocode_and_save(cached_records)
        else:
            print("âœ… æ‰€æœ‰è³‡æ–™éƒ½å·²åŒ…å«åº§æ¨™")
            return cached_records

# --------------------------------------------------------
# 5. åŒ¯å…¥è³‡æ–™ (å« PostGIS)
# --------------------------------------------------------

async def insert_records(records):
    database_url = os.environ.get("DATABASE_URL")
    if not database_url:
        raise ValueError("DATABASE_URL ç’°å¢ƒè®Šæ•¸æœªè¨­å®š")

    insert_query = """
        INSERT INTO shelters (
            shelter_code, name, city, district, address, capacity,
            area_m2, disasters, relief_station, barrier_free,
            indoor, outdoor, service_area, memo, geom
        )
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                ST_GeogFromText(%s))
        ON CONFLICT (shelter_code) DO NOTHING;
    """

    async with await psycopg.AsyncConnection.connect(database_url) as conn:
        async with conn.cursor() as cur:
            success_count = 0
            skip_count = 0
            
            for i, row in enumerate(records, start=1):
                try:
                    disasters = [name for name in ["æ°´ç½", "éœ‡ç½", "åœŸçŸ³æµ", "æµ·å˜¯"]
                                if row.get(name) and row[name] != "N"]

                    lat = row.get("lat")
                    lon = row.get("lon")
                    geom_wkt = f"POINT({lon} {lat})" if lat and lon else None
                    if not geom_wkt:
                        print(f"âš ï¸ Skip {row.get('åç¨±')} â€” no coordinates.")
                        skip_count += 1
                        continue

                    # è™•ç†å®¹ç´äººæ•¸ï¼Œç¢ºä¿æ˜¯æ•¸å€¼
                    try:
                        capacity = int(row.get("å®¹ç´äººæ•¸") or 0)
                    except (ValueError, TypeError):
                        capacity = 0
                    
                    # è™•ç†æ”¶å®¹æ‰€é¢ç©ï¼Œç¢ºä¿æ˜¯æ•¸å€¼
                    try:
                        area_str = str(row.get("æ”¶å®¹æ‰€é¢ç©ï¼ˆå¹³æ–¹å…¬å°ºï¼‰") or "0")
                        # ç§»é™¤å¯èƒ½çš„éæ•¸å€¼æ–‡å­—
                        if "æ”¹å»ºå¾Œé‡æ–°è©•ä¼°" in area_str or area_str.strip() == "":
                            area_m2 = 0.0
                        else:
                            area_m2 = float(area_str)
                    except (ValueError, TypeError):
                        area_m2 = 0.0

                    await cur.execute(insert_query, (
                        row.get("æ”¶å®¹æ‰€ç·¨è™Ÿ"),
                        row.get("åç¨±"),
                        row.get("ç¸£å¸‚"),
                        row.get("é„‰é®"),
                        row.get("é–€ç‰Œåœ°å€"),
                        capacity,
                        area_m2,
                        disasters,
                        row.get("æ•‘æ¿Ÿæ”¯ç«™") == "Y",
                        row.get("ç„¡éšœç¤™è¨­æ–½") == "Y",
                        row.get("å®¤å…§") == "Y",
                        row.get("å®¤å¤–") == "Y",
                        row.get("æœå‹™é‡Œåˆ¥"),
                        row.get("å‚™è€ƒ"),
                        geom_wkt
                    ))
                    print(f"âœ… {i}. {row.get('åç¨±')} â†’ {geom_wkt}")
                    success_count += 1
                    
                except Exception as e:
                    print(f"âš ï¸ {i}. {row.get('åç¨±', 'Unknown')} - æ’å…¥å¤±æ•—: {e}")
                    skip_count += 1
                    continue

            await conn.commit()
            print(f"ğŸ¯ æˆåŠŸåŒ¯å…¥ {success_count} ç­†è³‡æ–™ï¼Œè·³é {skip_count} ç­†ï¼ˆå…± {len(records)} ç­†ï¼‰")

# --------------------------------------------------------
# 6. ä¸»åŸ·è¡Œæµç¨‹
# --------------------------------------------------------

if __name__ == "__main__":
    print("=== é–‹å§‹åŸ·è¡Œé¿é›£æ‰€è³‡æ–™åŒ¯å…¥ç¨‹åº ===")
    
    try:
        # 1. å»ºç«‹è³‡æ–™è¡¨
        print("\næ­¥é©Ÿ 1: å»ºç«‹è³‡æ–™åº«è¡¨æ ¼...")
        asyncio.run(create_table())
        
        # 2. ç²å–å°åŒ—å¸‚é¿é›£æ‰€è³‡æ–™ï¼ˆåŒ…å«åº§æ¨™è™•ç†ï¼‰
        print("\næ­¥é©Ÿ 2: ç²å–å°åŒ—å¸‚é¿é›£æ‰€è³‡æ–™ä¸¦è™•ç†åº§æ¨™...")
        enriched_records = get_records_with_coordinates()
        
        if not enriched_records:
            print("âŒ æ²’æœ‰ç²å–åˆ°ä»»ä½•æœ‰æ•ˆè³‡æ–™")
            exit(1)
            
        # 3. å°‡è³‡æ–™æ’å…¥è³‡æ–™åº«
        print(f"\næ­¥é©Ÿ 3: å°‡ {len(enriched_records)} ç­†è³‡æ–™æ’å…¥è³‡æ–™åº«...")
        asyncio.run(insert_records(enriched_records))
        
        print("\n=== ç¨‹åºåŸ·è¡Œå®Œæˆ ===")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        exit(1)